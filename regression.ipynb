{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f60bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas xgboost catboost lightgbm tabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443070c3",
   "metadata": {},
   "source": [
    "declaring the dataframe and split the data to matrix X and vector Y\n",
    "\n",
    "A dataset for classification task the output is the income column.\n",
    "5000 rows, 12 cols 7 numerical features.\n",
    " you can find the dataset here: https://www.kaggle.com/datasets/ajinilpatel/energy-consumption-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ecbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.read_csv('Energy_consumption_dataset.csv')\n",
    "df_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e60e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd113157",
   "metadata": {},
   "source": [
    "### 8:\n",
    "Creating a preprocessing function:\n",
    "* I decided to create sub functions first and call them from a 'main' function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e971d84",
   "metadata": {},
   "source": [
    "A:  CHECKING FOR MISSING VALUES (THERE ISN'T MISSING VALUES) AND CATEGORICAL COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    print(\"Checking for missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    # df = df.fillna()  THERE IS NO NULL VALUES ANYWAY SO WE DON'T NEED THAT (:\n",
    "    return df\n",
    "\n",
    "def remove_high_cardinality_columns(df, max_unique=4):\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category','string']).columns\n",
    "    removed_cols = []\n",
    "\n",
    "    for col in cat_cols:\n",
    "        if df[col].nunique() > max_unique:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            removed_cols.append(col)\n",
    "\n",
    "    print(f\"Removed high-cardinality columns (>{max_unique} unique values): {removed_cols}\")\n",
    "    return df\n",
    "\n",
    "def encode_categorical_variables(df):\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "    print(f\"Encoded categorical columns: {list(cat_cols)}\")\n",
    "    return df_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f4bd8",
   "metadata": {},
   "source": [
    "B: SPLITTING FOR TRAIN/TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_reg(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c389f1f",
   "metadata": {},
   "source": [
    "C: STANDARDIZE THE DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e087916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features_reg(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Standardization complete.\")\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_regression_data(df, target_column):\n",
    "    X_reg = df.drop(columns=[target_column])\n",
    "    y_reg = df[target_column]\n",
    "    \n",
    "    X_reg = handle_missing_values(X_reg)\n",
    "    X_reg = remove_high_cardinality_columns(X_reg)\n",
    "    X_reg = encode_categorical_variables(X_reg)\n",
    "\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = split_data_reg(X_reg, y_reg)\n",
    "\n",
    "    X_train_scaled_reg, X_test_scaled_reg = standardize_features_reg(X_train_reg, X_test_reg)\n",
    "\n",
    "    return X_train_scaled_reg, X_test_scaled_reg, y_train_reg, y_test_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_reg, X_test_scaled_reg, y_train_reg, y_test_reg = preprocess_regression_data(df_reg, 'EnergyConsumption')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e3d87",
   "metadata": {},
   "source": [
    "### 9:\n",
    "Train a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model.fit(X_train_scaled_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a37e26",
   "metadata": {},
   "source": [
    "### 10: \n",
    "Making Predictions and evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom implementation of MAPE \n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set and train set\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled_reg)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled_reg)\n",
    "\n",
    "# Metrics:\n",
    "r2_train = r2_score(y_train_reg, y_train_pred_lr)\n",
    "rmse_train = mean_squared_error(y_train_reg, y_train_pred_lr)\n",
    "mae_train = mean_absolute_error(y_train_reg, y_train_pred_lr)\n",
    "mape_train = mean_absolute_percentage_error(y_train_reg, y_train_pred_lr)\n",
    "\n",
    "r2_test = r2_score(y_test_reg, y_test_pred_lr)\n",
    "rmse_test = mean_squared_error(y_test_reg, y_test_pred_lr)\n",
    "mae_test = mean_absolute_error(y_test_reg, y_test_pred_lr)\n",
    "mape_test = mean_absolute_percentage_error(y_test_reg, y_test_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Evaluation Metrics:\\n\")\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(f\"R^2: {r2_train:.4f}\")\n",
    "print(f\"RMSE: {rmse_train:.4f}\")\n",
    "print(f\"MAE: {mae_train:.4f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}%\\n\")\n",
    "\n",
    "print(\"Test Set:\")\n",
    "print(f\"R^2: {r2_test:.4f}\")\n",
    "print(f\"RMSE: {rmse_test:.4f}\")\n",
    "print(f\"MAE: {mae_test:.4f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fee000",
   "metadata": {},
   "source": [
    "### 11:\n",
    "Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fa3b7",
   "metadata": {},
   "source": [
    "Transforms features into polynomial features of the given degree.\n",
    "Returns a new DataFrame X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, degree):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    print(f\"Polynomial features created for degree {degree}. Shape: {X_poly.shape}\")\n",
    "    return X_poly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fc845",
   "metadata": {},
   "source": [
    "Splitting for train/test and train models by Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_polynomial_models(X, y, degrees=[2, 3, 4]):\n",
    "    results = []\n",
    "\n",
    "    for d in degrees:\n",
    "        print(f\"\\nEvaluating Polynomial Degree: {d}\")\n",
    "\n",
    "        # Create polynomial features\n",
    "        X_poly = create_polynomial_features(X, d)\n",
    "\n",
    "        # Split and scale\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "        X_train_scaled, X_test_scaled = standardize_features_reg(X_train, X_test)\n",
    "\n",
    "        # Ridge Regression with cross-validation\n",
    "        ridge_model = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)\n",
    "        ridge_model.fit(X_train_scaled, y_train)\n",
    "        y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "        r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "        # Lasso Regression with cross-validation\n",
    "        lasso_model = LassoCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5, max_iter=20000)\n",
    "        lasso_model.fit(X_train_scaled, y_train)\n",
    "        y_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "        r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "        results.append({\n",
    "            'degree': d,\n",
    "            'ridge_alpha': ridge_model.alpha_,\n",
    "            'lasso_alpha': lasso_model.alpha_,\n",
    "            'ridge_r2': r2_ridge,\n",
    "            'lasso_r2': r2_lasso,\n",
    "            'best_model': 'Ridge' if r2_ridge > r2_lasso else 'Lasso',\n",
    "            'best_r2': max(r2_ridge, r2_lasso)\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47477ff",
   "metadata": {},
   "source": [
    "Run the models and retrieve the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ec5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_polynomial_models(X_train_scaled_reg, y_train_reg)\n",
    "\n",
    "print(\"\\nSummary of Polynomial Models:\")\n",
    "for res in results:\n",
    "    print(f\"Degree {res['degree']}:\")\n",
    "    print(f\"  Ridge - Alpha: {res['ridge_alpha']} | R¬≤: {res['ridge_r2']:.4f}\")\n",
    "    print(f\"  Lasso - Alpha: {res['lasso_alpha']} | R¬≤: {res['lasso_r2']:.4f}\")\n",
    "    print(f\"  Best Model: {res['best_model']} with R¬≤: {res['best_r2']:.4f}\\n\")\n",
    "\n",
    "best = max(results, key=lambda x: x['best_r2'])\n",
    "print(\"üèÜ Best Overall Model:\")\n",
    "print(f\"Degree: {best['degree']}\")\n",
    "print(f\"Model: {best['best_model']}\")\n",
    "print(f\"R¬≤: {best['best_r2']:.4f}\")\n",
    "print(f\"Alpha: {best['ridge_alpha'] if best['best_model']=='Ridge' else best['lasso_alpha']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d741fe",
   "metadata": {},
   "source": [
    "üèÜ Best Overall Model:\n",
    "Degree: 2\n",
    "Model: Lasso\n",
    "R¬≤: 0.3340\n",
    "Alpha: 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c53b3",
   "metadata": {},
   "source": [
    "### 12:\n",
    "What can be the problem with creating variables in degree 2+ and how can you solve it?\n",
    "* the main problem is that the features grows significantly which can lead to overfitting that reduce the model's ability to generalize patterns.\n",
    "* We can solve it we use regularization methods such as Lasso and Ridge which we'll explain about them in the next question.\n",
    "\n",
    "What are Lasso and Ridge, when to use and what's the main difference between them?\n",
    "* Lasso and ridge are penalize large coefficients:\n",
    "* Use Lasso when you believe that some features are irrelevant, it shrinks those coefficients to zero and acting as a form of automatic **feature selection**\n",
    "* Use Ridge when you believe that all features contribute to the outcome, but you want to prevent any single feature from dominating. Ridge shrinks all coefficients without removing them.\n",
    "* Main Differences: \n",
    "* Lasso can eliminate features by setting their coefficients to zero (L1 regularization)\n",
    "* Ridge reduces the magnitude of all coefficients (L2 regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd76b0",
   "metadata": {},
   "source": [
    "### 13:\n",
    "Making prediction on train set and test set for the best model from 11 and display the metrics:\n",
    "\n",
    "üèÜ Best Overall Model:\n",
    "Degree: 2\n",
    "Model: Lasso\n",
    "R¬≤: 0.3340\n",
    "Alpha: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate polynomial features for degree 2\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_train_scaled_reg)\n",
    "\n",
    "# Split and scale again\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
    "    X_poly, y_train_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_poly_scaled, X_test_poly_scaled = standardize_features_reg(X_train_poly, X_test_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lasso = LassoCV(alphas=[0.1], cv=5, max_iter=20000)\n",
    "best_lasso.fit(X_train_poly_scaled, y_train_poly)\n",
    "\n",
    "y_train_pred_best_lasso = best_lasso.predict(X_train_poly_scaled)\n",
    "y_test_pred_best_lasso = best_lasso.predict(X_test_poly_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a35755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, label=\"Set\"):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    print(f\"{label} Evaluation:\")\n",
    "    print(f\"R^2: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on both sets\n",
    "evaluate_model(y_train_poly, y_train_pred_best_lasso, \"Training Set\")\n",
    "evaluate_model(y_test_poly, y_test_pred_best_lasso, \"Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470edd8d",
   "metadata": {},
   "source": [
    "### 14: \n",
    "Train models with gridsearchcv on train set from 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_gridsearch(model, param_grid, X_train, y_train):\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                        cv=5, scoring='r2', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_model = XGBRegressor()\n",
    "param_grid_XGB = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "best_XGBRegressor, best_XGB_params = train_model_with_gridsearch(\n",
    "    XGB_model, param_grid_XGB, X_train_scaled_reg, y_train_reg\n",
    ")\n",
    "print(\"Best XGBRegressor params:\", best_XGB_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_model = CatBoostRegressor(verbose=0)\n",
    "param_grid_CatBoost = {\n",
    "    'depth': [4, 6],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "best_CatBoostRegressor, best_CatBoost_params = train_model_with_gridsearch(\n",
    "    CatBoost_model, param_grid_CatBoost, X_train_scaled_reg, y_train_reg\n",
    ")\n",
    "print(\"Best CatBoostRegressor params:\", best_CatBoost_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_model = LGBMRegressor()\n",
    "param_grid_LGBM = {\n",
    "    'num_leaves': [31, 64],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "best_LGBMRegressor, best_LGBM_params = train_model_with_gridsearch(\n",
    "    LGBM_model, param_grid_LGBM, X_train_scaled_reg, y_train_reg\n",
    ")\n",
    "print(\"Best LGBMRegressor params:\", best_LGBM_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60994f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor()\n",
    "param_grid_RF = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10]\n",
    "}\n",
    "\n",
    "best_RFRegressor, best_RF_params = train_model_with_gridsearch(\n",
    "    RF_model, param_grid_RF, X_train_scaled_reg, y_train_reg\n",
    ")\n",
    "print(best_RF_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f694262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNRegressor\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "tabpfn_model = TabPFNRegressor(device='cuda' if torch.cuda.is_available() else 'cpu', ignore_pretraining_limits=True)\n",
    "\n",
    "# Fit on the training set (already scaled from step 8)\n",
    "tabpfn_model.fit(X_train_scaled_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed54657",
   "metadata": {},
   "source": [
    "### 15:\n",
    "Making predictions on train set and test set for each of the models and displaying the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_train, y_train_pred, y_test, y_test_pred, model_name):\n",
    "    print(f\"\\n{model_name} Evaluation\")\n",
    "\n",
    "    print(\"Training Set:\")\n",
    "    print(f\"R^2:   {r2_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"RMSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"MAE:  {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"MAPE: {mean_absolute_percentage_error(y_train, y_train_pred):.2f}%\")\n",
    "\n",
    "    print(\"\\nTest Set:\")\n",
    "    print(f\"R^2:   {r2_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"RMSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"MAE:  {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"MAPE: {mean_absolute_percentage_error(y_test, y_test_pred):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_xgb = best_XGBRegressor.predict(X_train_scaled_reg)\n",
    "y_test_pred_xgb = best_XGBRegressor.predict(X_test_scaled_reg)\n",
    "\n",
    "print_metrics(y_train_reg, y_train_pred_xgb, y_test_reg, y_test_pred_xgb, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ce6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_cat = best_CatBoostRegressor.predict(X_train_scaled_reg)\n",
    "y_test_pred_cat = best_CatBoostRegressor.predict(X_test_scaled_reg)\n",
    "\n",
    "print_metrics(y_train_reg, y_train_pred_cat, y_test_reg, y_test_pred_cat, \"CatBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_lgbm = best_LGBMRegressor.predict(X_train_scaled_reg)\n",
    "y_test_pred_lgbm = best_LGBMRegressor.predict(X_test_scaled_reg)\n",
    "\n",
    "print_metrics(y_train_reg, y_train_pred_lgbm, y_test_reg, y_test_pred_lgbm, \"LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_rf = best_RFRegressor.predict(X_train_scaled_reg)\n",
    "y_test_pred_rf = best_RFRegressor.predict(X_test_scaled_reg)\n",
    "\n",
    "print_metrics(y_train_reg, y_train_pred_rf, y_test_reg, y_test_pred_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e08be",
   "metadata": {},
   "source": [
    "Due to computational limitations, TabPFN predicted on a 1000-sample subset. While this doesn't allow a fully fair comparison, it offers insight into TabPFN's performance characteristics on limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small_reg = X_train_scaled_reg[:1000]\n",
    "y_train_small_reg = y_train_reg[:1000]\n",
    "\n",
    "X_test_small_reg = X_test_scaled_reg[:1000]\n",
    "y_test_small_reg = y_test_reg[:1000]\n",
    "\n",
    "y_train_tabpfn = tabpfn_model.predict(X_train_small_reg)\n",
    "y_test_tabpfn = tabpfn_model.predict(X_test_small_reg)\n",
    "print_metrics(y_train_small_reg, y_train_tabpfn, y_test_small_reg, y_test_tabpfn, \"tabPFN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180216cf",
   "metadata": {},
   "source": [
    "But I ran the predictions in VS code on my local GPU on the original data print_metrics(y_train_reg, y_train_pred_tabpfn, y_test_reg, y_test_pred_tabpfn, \"tabPFN\") and I markdown the results to be consistent with answer with my answer in 17.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN IT WITH GPU OTHERWISE IT STUCK\n",
    "'''\n",
    "y_train_pred_tabpfn = tabpfn_model.predict(X_train_scaled_reg)\n",
    "y_test_pred_tabpfn = tabpfn_model.predict(X_test_scaled_reg)\n",
    "print_metrics(y_train_reg, y_train_pred_tabpfn, y_test_reg, y_test_pred_tabpfn, \"tabPFN\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35059e97",
   "metadata": {},
   "source": [
    "### 16:\n",
    "Creating summary table with the 5 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'Model': ['XGBoost', 'CatBoost', 'LGBM', 'Random Forest', 'tabPFN','LinearRegression', 'Best Lasso'],\n",
    "    'RMSE Train': [\n",
    "        mean_squared_error(y_train_reg, y_train_pred_xgb),\n",
    "        mean_squared_error(y_train_reg, y_train_pred_cat),\n",
    "        mean_squared_error(y_train_reg, y_train_pred_lgbm),\n",
    "        mean_squared_error(y_train_reg, y_train_pred_rf),\n",
    "        mean_squared_error(y_train_small_reg, y_train_tabpfn),\n",
    "        mean_squared_error(y_train_reg, y_train_pred_lr),\n",
    "        mean_squared_error(y_train_poly, y_train_pred_best_lasso)\n",
    "    ],\n",
    "    \n",
    "    'RMSE Test': [\n",
    "        mean_squared_error(y_test_reg, y_test_pred_xgb),\n",
    "        mean_squared_error(y_test_reg, y_test_pred_cat),\n",
    "        mean_squared_error(y_test_reg, y_test_pred_lgbm),\n",
    "        mean_squared_error(y_test_reg, y_test_pred_rf),\n",
    "        mean_squared_error(y_test_reg, y_test_tabpfn),\n",
    "        mean_squared_error(y_test_reg, y_test_pred_lr),\n",
    "        mean_squared_error(y_test_poly, y_test_pred_best_lasso)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary).round(4)\n",
    "print(df_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b68ed",
   "metadata": {},
   "source": [
    "Marking down the table because i wanted to be consistent with my answer in 17.a\n",
    "\n",
    "\n",
    "| Model             | RMSE Train | RMSE Test |\n",
    "|-------------------|------------|-----------|\n",
    "| XGBOOST           | 40.8120    | 64.8693   |\n",
    "| CatBoost          | 52.1670    | 60.0235   |\n",
    "| LGBM              | 29.1277    | 63.2355   |\n",
    "| Random Forest     | 25.6229    | 62.6501   |\n",
    "| tabPFN            | 54.0252    | 59.8595   |\n",
    "| Linear Regression | 57.7122    | 61.0832   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dde3c6",
   "metadata": {},
   "source": [
    "### 17:\n",
    "\n",
    "17.a: \n",
    "\n",
    "\n",
    "Who is the best model?\n",
    "* Based on the RMSE values in the summary table, the best-performing model on the test set is tabPFN, with the lowest RMSE of 59.86.\n",
    "\n",
    "Explain shortly the models' results\n",
    "* XGBoostRegressor: The model performs well on training but significantly worse on test data, indicating overfitting - it learned the training set patterns too well, but doesn't generalize effectively.\n",
    " \n",
    "* CatBoostRegressor: A balanced result - the test RMSE is close to the training RMSE. This suggests good generalization and a solid trade-off between bias and variance. One of the more stable models I got.\n",
    "\n",
    "* LGBMRegressor: Very low training error but much higher error on the test set - a classic case of overfitting. The model likely memorized the training data and doesn't generalize well.\n",
    "\n",
    "* RFRegressor: Extremely low training error and a large gap with test error - strong overfitting. While it excels on the training data, it performs poorly on unseen data.\n",
    "\n",
    "* tabPFNRegressor: Balanced performance. The small gap between train and test RMSE shows that tabPFN generalizes well. Among all models, it has the lowest test RMSE, making it the best performer overall in terms of predictive accuracy.\n",
    "\n",
    "* This model is very stable, with a small difference between training and test errors. It slightly overfit, but its test RMSE is higher than tabPFN and CatBoost, meaning it is reliable but less accurate.\n",
    "\n",
    "Explain each of the 4 evaluation metrics.\n",
    "* R^2 measures how well the model explains the variance in the target variable, it ranges from 0 to 1 where higher values indicate a better fit.\n",
    "In the context of energy consumption prediction, a high R¬≤ means the model successfully captures the patterns in features like temperature and occupancy to explain fluctuations in energy usage. \n",
    "* RMSE is measured in the same units as the target variable - in this case, kilowatt-hours (kWh) ‚Äî making it highly interpretable. It calculates the square root of the average squared difference between predicted and actual values, which means it penalizes larger errors more heavily than smaller ones. This makes RMSE especially useful in my case: when predicting energy usage, a few large mistakes (e.g., underestimating consumption during peak hours) can have operational or financial consequences. Therefore, RMSE is a key metric in this task, as it reflects both average error and sensitivity to outliers.\n",
    "* MAE is also measured in the same units as the target variable and represents the average absolute difference between the predicted and actual values. Unlike RMSE, MAE treats all errors equally, regardless of their size, making it more robust to outliers. In the context of this project, MAE tells, on average, how many kWh (EnergyConsumption) your predictions deviate from the true values. This is useful when you want a straightforward, reliable estimate of how far off the model tends to be, especially if both small and large errors are equally important to your stakeholders.\n",
    "\n",
    "* MAPE expresses error as a percentage of the actual values, making it unitless and easy to interpret across different datasets or use cases. For example, a MAPE of 8% means your model is off by 8% on average, or conversely, 92% accurate. This is particularly helpful for communicating model performance to non-technical audiences. In this project, MAPE allows you to quantify how reliable the energy predictions are in relative terms - a key consideration if different buildings or systems have widely varying levels of energy consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83be12f",
   "metadata": {},
   "source": [
    "17.b:\n",
    "Related to this context, which is the best suited metric?\n",
    "\n",
    "* RMSE is the most appropriate and informative metric. Here's why:\n",
    "\n",
    "RMSE is measured in the same units as the target variable - kilowatt-hours (kWh) - and it penalizes large errors more heavily than smaller ones. This is particularly important in energy forecasting, where a few significant underestimations or overestimations can lead to real-world problems, such as power shortages, overproduction, or inefficient energy distribution. Unlike MAE, which treats all errors equally, RMSE gives more weight to large deviations - and in this context, such deviations can have operational and financial consequences.\n",
    "\n",
    "Additionally, RMSE is commonly used in energy analytics and engineering applications for its sensitivity to peak error scenarios, making it a natural fit when modeling and optimizing for resource consumption.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
