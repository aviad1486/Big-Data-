{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas xgboost catboost lightgbm tabPFN imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c3250",
   "metadata": {},
   "source": [
    "### 1: \n",
    "Importing the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccac877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc3ecb",
   "metadata": {},
   "source": [
    "### 2:\n",
    "load the data and declaring the dataframe and split the data to matrix X and vector Y (I'll do it later as a part of step 3)\n",
    "\n",
    "A dataset for classification task the output is the income column.\n",
    "32561 rows, 15 cols 6 numerical features.\n",
    " you can find the dataset here: https://www.kaggle.com/datasets/uciml/adult-census-income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f25656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv')\n",
    "print(df.shape)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb52ce3",
   "metadata": {},
   "source": [
    "### 3:\n",
    "Creating a preprocessing function:\n",
    "* I decided to create sub functions first and call them from a 'main' function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4cbc1",
   "metadata": {},
   "source": [
    "A:  CHECKING FOR MISSING VALUES (THERE ISN'T MISSING VALUES) AND CATEGORICAL COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31240962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15910795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    print(\"Checking for missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    # df = df.fillna()  THERE IS NO NULL VALUES ANYWAY SO WE DON'T NEED THAT (:\n",
    "    return df\n",
    "\n",
    "def drop_high_cardinality_categorical_features(X, threshold=5):\n",
    "    categorical_cols = X.select_dtypes(include='object').columns\n",
    "    high_card_cols = [col for col in categorical_cols if X[col].nunique() > threshold]\n",
    "    \n",
    "    print(f\"ðŸ§¹ Dropping high-cardinality categorical columns: {high_card_cols}\")\n",
    "    X = X.drop(columns=high_card_cols)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def convert_categorical_to_dummies(X):\n",
    "    print(\"Converting categorical variables to dummy variables...\")\n",
    "    X = pd.get_dummies(X, drop_first=True) \n",
    "    print(f\"New shape after encoding: {X.shape}\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15e813",
   "metadata": {},
   "source": [
    "B: REMOVING DUPLICATES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0b101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_rows(X, y):\n",
    "    print(\"Initial number of rows:\", len(X))\n",
    "    \n",
    "    # Combine X and y to detect and drop duplicates together\n",
    "    df_combined = pd.concat([X, y], axis=1)\n",
    "    df_combined_no_duplicates = df_combined.drop_duplicates()\n",
    "\n",
    "    # Separate X and y again\n",
    "    X_cleaned = df_combined_no_duplicates.drop(columns=[y.name])\n",
    "    y_cleaned = df_combined_no_duplicates[y.name]\n",
    "\n",
    "    print(\"Number of rows after removing duplicates:\", len(X_cleaned))\n",
    "    return X_cleaned, y_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51437e93",
   "metadata": {},
   "source": [
    "C: SPLITTING FOR TRAIN/TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e5e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Testing set size: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231915d5",
   "metadata": {},
   "source": [
    "D: STANDARDIZE THE DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b835dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(X_train, X_test):\n",
    "    print(\"Standardizing feature columns...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Standardization complete.\")\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856cd23",
   "metadata": {},
   "source": [
    "E: Handle Class Imbalance:\n",
    "* There was a need to balance the data and I chose to oversampling the minority class because it's simple and avoids losing data.\n",
    "* The advantages of this method are: preventing the model from being biased toward the majority class, improving recall and F1-score for the minority class, and it works well on decision tree and random forest which I will use later in this project.\n",
    "* We use this method when the dataset is small, when we want to avoid losing information, and when the minority class have high importance for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd1773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_imbalance(y):\n",
    "    print(\"Class distribution:\")\n",
    "    print(y.value_counts())\n",
    "    print(\"\\nPercentage distribution:\")\n",
    "    print(round(y.value_counts(normalize=True) * 100, 2))\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    y.value_counts().plot(kind='bar', color='skyblue')\n",
    "    plt.title('Target Class Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0956688e",
   "metadata": {},
   "source": [
    "F: MAKE THE FUNCTION CALLS BY THE ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f45e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_classification_pipeline(df, target_column):\n",
    "    # Step 1: Split to X and y\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Step 2: Handle missing values in X\n",
    "    X = check_missing_values(X)\n",
    "\n",
    "    # Step 3: Remove high-cardinality categorical columns from X\n",
    "    X = drop_high_cardinality_categorical_features(X)\n",
    "\n",
    "    # Step 4: Convert categorical columns in X to dummy variables\n",
    "    X = convert_categorical_to_dummies(X)\n",
    "\n",
    "    # Step 5: Remove duplicate rows from X and y \n",
    "    X, y = remove_duplicate_rows(X, y)\n",
    "\n",
    "    # Step 6: Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Step 7: Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Step 8: Analyze class imbalance\n",
    "    analyze_class_imbalance(y) \n",
    "\n",
    "    # Step 9: Imbalance detected: Oversample the minority class in the training set and stardize again\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 10: Report final class distribution\n",
    "    print(\"\\nFinal class distribution in training set:\")\n",
    "    print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = main_classification_pipeline(df, target_column='income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08d6d5",
   "metadata": {},
   "source": [
    "### 4:\n",
    "Train classification models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.dtype)\n",
    "print(y_train.unique())\n",
    "\n",
    "# Needed because the target variable is an object type.\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test) \n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e90600",
   "metadata": {},
   "source": [
    "XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(eval_metric='logloss')\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 8, None]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_xgb_model = xgb_grid.best_estimator_\n",
    "print(\"Best XGBoost Parameters:\", xgb_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b99ce",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_rf_model = rf_grid.best_estimator_\n",
    "print(\"Best Random Forest Parameters:\", rf_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa5241",
   "metadata": {},
   "source": [
    "Catboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f05f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = CatBoostClassifier(verbose=0)\n",
    "cat_param_grid = {\n",
    "    'depth': [6, 10, 12],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(cat_model, cat_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cat_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_cat_model = cat_grid.best_estimator_\n",
    "print(\"Best CatBoost Parameters:\", cat_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e231b2",
   "metadata": {},
   "source": [
    "LGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba91870",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMClassifier()\n",
    "lgbm_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "lgbm_grid = GridSearchCV(lgbm_model, lgbm_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "lgbm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_lgbm_model = lgbm_grid.best_estimator_\n",
    "print(\"Best LightGBM Parameters:\", lgbm_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d1666",
   "metadata": {},
   "source": [
    "TabPFN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabpfn_model = TabPFNClassifier(device='cuda' if torch.cuda.is_available() else 'cpu',ignore_pretraining_limits=True)\n",
    "\n",
    "# Fit on the smaller training set\n",
    "tabpfn_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcfc59",
   "metadata": {},
   "source": [
    "BernouliRBM\n",
    "* I used this documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159201da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing + RBM + LogisticRegression pipeline\n",
    "rbm_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),  # Optional but recommended\n",
    "    ('rbm', BernoulliRBM(random_state=42)),\n",
    "    ('log_reg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "rbm_params = {\n",
    "    'rbm__n_components': [32, 64],\n",
    "    'log_reg__C': [0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Create and run GridSearchCV\n",
    "rbm_grid = GridSearchCV(rbm_pipeline, rbm_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "rbm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model and parameters\n",
    "best_rbm_model = rbm_grid.best_estimator_\n",
    "print(\"Best RBM + Logistic Regression Parameters:\", rbm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34a67d",
   "metadata": {},
   "source": [
    "### 5:\n",
    "Building a NN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db919c10",
   "metadata": {},
   "source": [
    "I built a 3-layer feedforward NN with the following structure:\n",
    "* Dense layer - 64 neurons - with ReLu activation function - BatchNormalization (https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) for faster convergence - and Dropout(0.3).\n",
    "* Dense - 32 neurons - ReLU - Dropout(0.2)\n",
    "* Dense - 1 neuron + Sigmoid (for 2-class classification ('income'))\n",
    "I chose this architecture in order to prevent overfitting using dropout and earlystopping, speed-up training using BatchNormalization, Ensure non linearity with ReLu.\n",
    "\n",
    "Related to my data I chose this architecture because my data is medium size and this architecture made the model optimized for binary classification and the feature space is not high dimensional after 'clean_categorical_columns' method so i try a simple and shallow version for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b566b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(X_train_scaled.shape[1],), activation='relu'),\n",
    "    BatchNormalization(trainable = False),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(1, activation='sigmoid')  # For binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12de4fe",
   "metadata": {},
   "source": [
    "### 6: \n",
    "Making Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836854c9",
   "metadata": {},
   "source": [
    "A: Making Predicitons on the models I created before on train set and test set and print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, is_nn=False):\n",
    "    print(f\"\\nðŸ“Œ {name} â€“ Train Performance:\")\n",
    "    y_train_pred = (model.predict(X_train) > 0.5).astype(int).flatten() if is_nn else model.predict(X_train) # CAN'T PRINT CLASSIFICATION REPORT ON CONTINOUS NUMBERS IN NN\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    print(f\"ðŸ“Œ {name} â€“ Test Performance:\")\n",
    "    y_test_pred = (model.predict(X_test) > 0.5).astype(int).flatten() if is_nn else model.predict(X_test)\n",
    "    print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"XGBoost\", best_xgb_model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"Random Forest\", best_rf_model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"Catboost\", best_cat_model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"LightGBM\", best_lgbm_model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fffb5",
   "metadata": {},
   "source": [
    "Due to computational limitations, TabPFN was evaluated on a 1000-sample subset. While this doesn't allow a fully fair comparison, it offers insight into TabPFN's performance characteristics on limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc23c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = X_train_scaled[:1000]\n",
    "y_train_small = y_train[:1000]\n",
    "\n",
    "X_test_small = X_test_scaled[:1000]\n",
    "y_test_small = y_test[:1000]\n",
    "\n",
    "evaluate_model(\"TabPFN\", tabpfn_model, X_train_small, y_train_small, X_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dcd3f",
   "metadata": {},
   "source": [
    "But I ran the evaluate_model in VS code on the original data (X_train_scaled, y_train, X_test_scaled, y_test) and I markdown the results to be consistent with answer 6.B + 6.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811241fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model(\"TabPFN\", tabpfn_model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f79d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"BernouliRBM\", best_rbm_model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e42158",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"NN model\", model, X_train_scaled, y_train, X_test_scaled, y_test,is_nn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b7b0cd",
   "metadata": {},
   "source": [
    "B: Based on accuracy explain who is the best model, regard to train/test prediction, and use overfitting-underfitting terms:\n",
    "* Based on the accuracy metric, the best-performing model is the Neural network, which achieved 81% accuracy on the test set and 77% on the training set. The relatively small gap between the two sets suggests that the model does not suffer from overfitting or underfitting. Instead, it demonstrates good generalization closing to appropriate fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d68e61",
   "metadata": {},
   "source": [
    "C: Based on other metric explain who is the best model, regard to train/test prediction, explain the meaning of the metric and why does it relevant to this prediction task:\n",
    "* When considering the F1-score for class 1, the CatBoost model demonstrates the best performance, achieving an F1-score of 0.87 on the training set and 0.65 on the test set. The F1-score is the harmonic mean of precision and recall, and is especially valuable in classification tasks involving imbalanced datasets â€” like in our case, where the minority class (>50K) represents only about 24% of the data. \n",
    "if we wrongly predict a user as being the highest income group when he's not, it could lead to unfair decisions like offering services they don't qualify for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a5762",
   "metadata": {},
   "source": [
    "### 7:\n",
    "A: Improving the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb10baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_improved = Sequential([\n",
    "    Dense(128, input_shape=(X_train_scaled.shape[1],), activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(64, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(32, activation='tanh'),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_improved.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_improved = model_improved.fit( \n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae7635",
   "metadata": {},
   "source": [
    "Showing the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"Neural Network\", model, X_train_scaled, y_train, X_test_scaled, y_test,is_nn = True)\n",
    "evaluate_model(\"Enhanced Neural Network\", model_improved, X_train_scaled, y_train, X_test_scaled, y_test,is_nn = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b7eeb",
   "metadata": {},
   "source": [
    "7.b:\n",
    "\n",
    "The original neural network already showed good performance, but several changes were made to improve generalization. The activation function was switched to tanh for smoother convergence, and the number of neurons and layers was increased to capture more complex patterns. Batch normalization was added for stability, dropout was reduced since overfitting wasnâ€™t an issue, and the validation split was increased for better performance monitoring.  These modifications were aimed at pushing an already strong model to perform even better as reflected by the improved evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
